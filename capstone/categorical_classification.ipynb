{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dense, Embedding, Bidirectional, LSTM\n",
    "from tensorflow.keras.layers import Dropout, Activation\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils.data_utils import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### used_data 필요한 피처들을 합친 데이터 프레임\n",
    "- findings + 5개 label 피처 합침\n",
    "---\n",
    "merged_label_conditons 파일\n",
    "14개의 피처를 5개의 관련 질환으로 재분류 한 라벨 데이터\n",
    "- 폐 질환, 순환기 질환, 흉막 관련 질환, 관절 관련 질환, 발견 질환 없음\n",
    "- label로 사용할 데이터 -> num_classes = 5\n",
    "---\n",
    "fidngs_and_impression 파일\n",
    "흉부영상에서 발견한 특이점을 기록한 findings 피쳐가 있는 파일\n",
    "- findings 피처를 텍스트 전처리한 다음 입력 값으로 사용\n",
    "- input_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>study_id</th>\n",
       "      <th>LungConditions</th>\n",
       "      <th>CardiacAndCirculatoryConditions</th>\n",
       "      <th>PleuralConditions</th>\n",
       "      <th>SkeletalConditions</th>\n",
       "      <th>NormalConditions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50414267</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53189527</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53911762</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56699142</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57375967</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   study_id  LungConditions  CardiacAndCirculatoryConditions  \\\n",
       "0  50414267               0                                0   \n",
       "1  53189527               0                                0   \n",
       "2  53911762               0                                0   \n",
       "3  56699142               0                                0   \n",
       "4  57375967               1                                0   \n",
       "\n",
       "   PleuralConditions  SkeletalConditions  NormalConditions  \n",
       "0                  0                   0                 1  \n",
       "1                  0                   0                 1  \n",
       "2                  0                   0                 1  \n",
       "3                  0                   0                 1  \n",
       "4                  0                   0                 0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_dir = './data'\n",
    "categorical_label_data_path = base_dir + '/merged_label_conditions.csv'\n",
    "ehr_csv_data_path = base_dir + '/findings_and_impression.csv'\n",
    "\n",
    "categorical_data = pd.read_csv(categorical_label_data_path)\n",
    "text_data = pd.read_csv(ehr_csv_data_path)\n",
    "\n",
    "finding_data = text_data[['study_id', 'Findings']]\n",
    "used_data = categorical_data.merge(finding_data, how='inner')\n",
    "\n",
    "used_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 결측치 제거\n",
    "2. 특수문자 제거\n",
    "3. 불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is no focal consolidation pleural effusion or pneumothorax bilateral nodular opacities that most likely represent nipple shadows the cardiomediastinal silhouette is normal clips project over the left lung potentially within the breast the imaged upper abdomen is unremarkable chronic deformity of the posterior left sixth and seventh ribs are noted \n",
      "focal consolidation pleural effusion pneumothorax bilateral nodular opacities likely represent nipple shadows cardiomediastinal silhouette normal clips project left lung potentially within breast imaged upper abdomen unremarkable chronic deformity posterior left sixth seventh ribs noted \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## 결측치 제거\n",
    "used_data = used_data.dropna()\n",
    "\n",
    "## 리스트 형식으로 변환\n",
    "findings_lst = []\n",
    "used_data['Findings'].astype('str')\n",
    "for string in used_data['Findings']:\n",
    "    findings_lst.append(string)\n",
    "\n",
    "## 특수문자 제거\n",
    "def re_text_preprocessing(lst):\n",
    "     re_cleaned_list = []\n",
    "     for text in lst:\n",
    "          text = text.lower()\n",
    "          words = text.split()\n",
    "          refine_text = \"\"\n",
    "          for word in words:\n",
    "               refine_text += word + ' '\n",
    "          re_cleaned_text = re.sub(r'[^\\w\\s]', '', refine_text)\n",
    "          re_cleaned_list.append(re_cleaned_text)\n",
    "     return re_cleaned_list\n",
    "\n",
    "filtered_fidindgs = re_text_preprocessing(findings_lst)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "## 불용어 제거\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def remove_stopwords(texts):\n",
    "    rm_sw_text = \"\"\n",
    "    words = texts.split()\n",
    "    filtered_text = \"\"\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            filtered_text += word + ' '\n",
    "    rm_sw_text += filtered_text\n",
    "    return rm_sw_text\n",
    "\n",
    "removed_sw_findings = []\n",
    "for idx in range(len(filtered_fidindgs)):\n",
    "    removed_sw_findings.append(remove_stopwords(filtered_fidindgs[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 라벨 데이터 원 - 핫 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 14509 entries, 0 to 22188\n",
      "Data columns (total 5 columns):\n",
      " #   Column                           Non-Null Count  Dtype\n",
      "---  ------                           --------------  -----\n",
      " 0   LungConditions                   14509 non-null  int64\n",
      " 1   CardiacAndCirculatoryConditions  14509 non-null  int64\n",
      " 2   PleuralConditions                14509 non-null  int64\n",
      " 3   SkeletalConditions               14509 non-null  int64\n",
      " 4   NormalConditions                 14509 non-null  int64\n",
      "dtypes: int64(5)\n",
      "memory usage: 680.1 KB\n"
     ]
    }
   ],
   "source": [
    "## 원핫 인코딩\n",
    "labels = ['LungConditions', 'CardiacAndCirculatoryConditions', 'PleuralConditions', \n",
    "          'SkeletalConditions', 'NormalConditions']\n",
    "\n",
    "multiple_labels = pd.get_dummies(used_data[labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dmis-lab biobert를 이용한 단어 토큰화 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1469\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 의료계에서 사용하는 토큰화 모델\n",
    "tokenizer_model_name = 'dmis-lab/biobert-base-cased-v1.2'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_model_name)\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "tokens_list = []\n",
    "for text in removed_sw_findings:\n",
    "    tokens = tokenizer.encode(text, max_length=512)\n",
    "    tokens_list.append(tokens)\n",
    "\n",
    "tokens_len = []\n",
    "for text_len in removed_sw_findings:\n",
    "    tokens_len.append(len(text_len))\n",
    "\n",
    "print(max(tokens_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력값과 라벨 최종 생성 완료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14509, 1469)\n",
      "(14509, 5)\n"
     ]
    }
   ],
   "source": [
    "max_seq = max(tokens_len)\n",
    "\n",
    "tokens_list_padded = pad_sequences(tokens_list, padding='pre', maxlen=max_seq)\n",
    "input_value = tokens_list_padded\n",
    "\n",
    "label = multiple_labels\n",
    "\n",
    "print(input_value.shape)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11607, 1469) (2902, 1469) (11607, 5) (2902, 5)\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "\n",
    "input_train, input_test, label_train, label_test = train_test_split(input_value, label, test_size=0.2, random_state=seed)\n",
    "\n",
    "print(input_train.shape, input_test.shape, label_train.shape, label_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sub_lstm(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_units, lstm_units):\n",
    "        super(sub_lstm, self).__init__()\n",
    "\n",
    "        self.embedding_layer = Embedding(vocab_size, embedding_units)\n",
    "\n",
    "        self.lstm_layer = LSTM(lstm_units)\n",
    "\n",
    "        self.output_layer = Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        inputs = tf.cast(inputs, dtype=tf.float32)\n",
    "\n",
    "        x = self.embedding_layer(inputs)\n",
    "\n",
    "        x = self.lstm_layer(x)\n",
    "\n",
    "        outputs = self.output_layer(x)\n",
    "\n",
    "        return outputs     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
